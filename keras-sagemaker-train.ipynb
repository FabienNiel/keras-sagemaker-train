{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training Job \n",
    "\n",
    "### Please go through this notebook only if you have finished Part 1 to Part 4 of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 1: Import packages, get IAM role, get the region and set the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket ='keras-sagemaker-train' # Put your s3 bucket name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 2: Create the algorithm image and push to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Stopping docker: [  OK  ]\r\n",
      "Starting docker:\t.[  OK  ]\r\n",
      "Sending build context to Docker daemon  11.47MB\r",
      "\r\n",
      "Step 1/6 : FROM phenompeople/centos-python:3.6.3\n",
      "3.6.3: Pulling from phenompeople/centos-python\n",
      "7dc0dca2b151: Pulling fs layer\n",
      "4e13d20dd920: Pulling fs layer\n",
      "4e03286a7322: Pulling fs layer\n",
      "1460c005753b: Pulling fs layer\n",
      "b6d4fd0c5aa4: Pulling fs layer\n",
      "b291b7062d5c: Pulling fs layer\n",
      "1460c005753b: Waiting\n",
      "b6d4fd0c5aa4: Waiting\n",
      "b291b7062d5c: Waiting\n",
      "4e03286a7322: Verifying Checksum\n",
      "4e03286a7322: Download complete\n",
      "4e13d20dd920: Verifying Checksum\n",
      "4e13d20dd920: Download complete\n",
      "7dc0dca2b151: Verifying Checksum\n",
      "7dc0dca2b151: Download complete\n",
      "1460c005753b: Verifying Checksum\n",
      "1460c005753b: Download complete\n",
      "b291b7062d5c: Verifying Checksum\n",
      "b291b7062d5c: Download complete\n",
      "b6d4fd0c5aa4: Verifying Checksum\n",
      "b6d4fd0c5aa4: Download complete\n",
      "7dc0dca2b151: Pull complete\n",
      "4e13d20dd920: Pull complete\n",
      "4e03286a7322: Pull complete\n",
      "1460c005753b: Pull complete\n",
      "b6d4fd0c5aa4: Pull complete\n",
      "b291b7062d5c: Pull complete\n",
      "Digest: sha256:860522cc90cb56ce2c033153dcc7ec6079189972dbbe6c28c46fb420692b038e\n",
      "Status: Downloaded newer image for phenompeople/centos-python:3.6.3\n",
      " ---> e3d7d8ca4a30\n",
      "Step 2/6 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Running in b88276ce5458\n",
      "Removing intermediate container b88276ce5458\n",
      " ---> 470f34e723ba\n",
      "Step 3/6 : ADD requirements-cpu.txt /\n",
      " ---> a5699ea2a40c\n",
      "Step 4/6 : RUN pip3 install -r requirements-cpu.txt\n",
      " ---> Running in 34cddd85d6f6\n",
      "Collecting absl-py==0.7.1 (from -r requirements-cpu.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "Collecting astor==0.8.0 (from -r requirements-cpu.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting gast==0.2.2 (from -r requirements-cpu.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting grpcio==1.21.1 (from -r requirements-cpu.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/99/83/18f374294bf34128a448ee2fae37651f943b0b5fa473b5b3aff262c15bf8/grpcio-1.21.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "Collecting h5py==2.9.0 (from -r requirements-cpu.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
      "Collecting Keras==2.2.4 (from -r requirements-cpu.txt (line 6))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Collecting Keras-Applications==1.0.8 (from -r requirements-cpu.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Collecting Keras-Preprocessing==1.1.0 (from -r requirements-cpu.txt (line 8))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Collecting Markdown==3.1.1 (from -r requirements-cpu.txt (line 9))\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Collecting mock==3.0.5 (from -r requirements-cpu.txt (line 10))\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting numpy==1.16.4 (from -r requirements-cpu.txt (line 11))\n",
      "  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "Collecting protobuf==3.8.0 (from -r requirements-cpu.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/fb/29de8d08967f0cce1bb10b39846d836b0f3bf6776ddc36aed7c73498ca7e/protobuf-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
      "Collecting PyYAML==5.1.1 (from -r requirements-cpu.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
      "Collecting scipy==1.3.0 (from -r requirements-cpu.txt (line 14))\n",
      "  Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
      "Collecting six==1.12.0 (from -r requirements-cpu.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard==1.13.1 (from -r requirements-cpu.txt (line 16))\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "Collecting tensorflow==1.13.1 (from -r requirements-cpu.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
      "Collecting tensorflow-estimator==1.13.0 (from -r requirements-cpu.txt (line 18))\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "Collecting termcolor==1.1.0 (from -r requirements-cpu.txt (line 19))\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting Werkzeug==0.15.4 (from -r requirements-cpu.txt (line 20))\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/57/92a497e38161ce40606c27a86759c6b92dd34fcdb33f64171ec559257c02/Werkzeug-0.15.4-py2.py3-none-any.whl (327kB)\n",
      "Collecting setuptools>=36 (from Markdown==3.1.1->-r requirements-cpu.txt (line 9))\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl (575kB)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/site-packages (from tensorboard==1.13.1->-r requirements-cpu.txt (line 16))\n",
      "Building wheels for collected packages: absl-py, gast, PyYAML, termcolor\n",
      "  Running setup.py bdist_wheel for absl-py: started\n",
      "  Running setup.py bdist_wheel for absl-py: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Running setup.py bdist_wheel for gast: started\n",
      "  Running setup.py bdist_wheel for gast: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Running setup.py bdist_wheel for PyYAML: started\n",
      "  Running setup.py bdist_wheel for PyYAML: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
      "  Running setup.py bdist_wheel for termcolor: started\n",
      "  Running setup.py bdist_wheel for termcolor: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built absl-py gast PyYAML termcolor\n",
      "Installing collected packages: six, absl-py, astor, gast, grpcio, numpy, h5py, PyYAML, Keras-Applications, Keras-Preprocessing, scipy, Keras, setuptools, Markdown, mock, protobuf, Werkzeug, tensorboard, tensorflow-estimator, termcolor, tensorflow\n",
      "  Found existing installation: setuptools 28.8.0\n",
      "    Uninstalling setuptools-28.8.0:\n",
      "      Successfully uninstalled setuptools-28.8.0\n",
      "Successfully installed Keras-2.2.4 Keras-Applications-1.0.8 Keras-Preprocessing-1.1.0 Markdown-3.1.1 PyYAML-5.1.1 Werkzeug-0.15.4 absl-py-0.7.1 astor-0.8.0 gast-0.2.2 grpcio-1.21.1 h5py-2.9.0 mock-3.0.5 numpy-1.16.4 protobuf-3.8.0 scipy-1.3.0 setuptools-41.0.1 six-1.12.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0\n",
      "\u001b[91mYou are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 34cddd85d6f6\n",
      " ---> 51c5932bc0ab\n",
      "Step 5/6 : COPY src /opt/program\n",
      " ---> 50a2c12fc8f7\n",
      "Step 6/6 : WORKDIR /opt/program\n",
      " ---> Running in ac25d42c9bc0\n",
      "Removing intermediate container ac25d42c9bc0\n",
      " ---> 95c9b3aa81f0\n",
      "Successfully built 95c9b3aa81f0\n",
      "Successfully tagged keras-sagemaker-train:latest\n",
      "The push refers to repository [850021735523.dkr.ecr.us-east-1.amazonaws.com/keras-sagemaker-train]\n",
      "9f12d555da42: Preparing\n",
      "236bbdb0679d: Preparing\n",
      "95c677b3c980: Preparing\n",
      "952e0784686f: Preparing\n",
      "65c06ae44bbd: Preparing\n",
      "f194f1dd3e8f: Preparing\n",
      "ea264623c568: Preparing\n",
      "c4cd48200f79: Preparing\n",
      "bcc97fbfc9e1: Preparing\n",
      "f194f1dd3e8f: Waiting\n",
      "ea264623c568: Waiting\n",
      "c4cd48200f79: Waiting\n",
      "bcc97fbfc9e1: Waiting\n",
      "65c06ae44bbd: Layer already exists\n",
      "952e0784686f: Layer already exists\n",
      "f194f1dd3e8f: Layer already exists\n",
      "ea264623c568: Layer already exists\n",
      "c4cd48200f79: Layer already exists\n",
      "bcc97fbfc9e1: Layer already exists\n",
      "95c677b3c980: Pushed\n",
      "9f12d555da42: Pushed\n",
      "236bbdb0679d: Pushed\n",
      "latest: digest: sha256:cd0964d1cc64d93e4cd684d2936110b1e270c4b6272b5619b6da11d05b51548c size: 2213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=keras-sagemaker-train\n",
    "\n",
    "chmod +x src/*\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "# Comment the line below to use a GPU\n",
    "docker build  -t ${algorithm_name} -f Dockerfile.cpu .\n",
    "\n",
    "# Uncomment the below line if you wish to run on a GPU\n",
    "#docker build  -t ${algorithm_name} -f Dockerfile.gpu . \n",
    "\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 3: Define variables with data location and output location in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data location - s3://keras-sagemaker-train/data\n",
      "output location - s3://keras-sagemaker-train/output\n"
     ]
    }
   ],
   "source": [
    "data_location = 's3://{}/data'.format(bucket)\n",
    "print(\"data location - \" + data_location)\n",
    "\n",
    "output_location = 's3://{}/output'.format(bucket)\n",
    "print(\"output location - \" + output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 4: Create a SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 5: Define variables for account, region and algorithm image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account'] # aws account \n",
    "region = sess.boto_session.region_name # aws server region\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/keras-sagemaker-train'.format(account, region) # algorithm image path in ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 6: Define hyperparameters to be passed to your algorithm. \n",
    "In this project we are reading two hyperparameters for training. Use of hyperparameters in optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"batch_size\":128, \"epochs\":30}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 7: Create the training job using SageMaker Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sage.estimator.Estimator(image_name=image, \n",
    "                                      role=role,\n",
    "                                      train_instance_count=1, \n",
    "                                      train_instance_type='ml.c5.2xlarge',\n",
    "                                      hyperparameters=hyperparameters,\n",
    "                                      output_path=output_location,\n",
    "                                      sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 8: Run the training job by passing the data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-14 09:15:48 Starting - Starting the training job...\n",
      "2019-06-14 09:15:51 Starting - Launching requested ML instances......\n",
      "2019-06-14 09:17:02 Starting - Preparing the instances for training...\n",
      "2019-06-14 09:17:43 Downloading - Downloading input data\n",
      "2019-06-14 09:17:43 Training - Downloading the training image..."
     ]
    }
   ],
   "source": [
    "classifier.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! We had a successful training job run in Amazon SageMaker.\n",
    "#### Please return to the tutorial for Part 6 where we will be running a training job in a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
