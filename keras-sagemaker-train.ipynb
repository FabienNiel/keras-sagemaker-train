{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-us-east-1.amazonaws.com/sagemaker-keras-sagemaker-train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket='sagemaker-keras-sagemaker-train' # Put your s3 bucket name here\n",
    "# customize to your bucket where you will store data\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)\n",
    "print(bucket_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Stopping docker: [  OK  ]\r\n",
      "Starting docker:\t.[  OK  ]\r\n",
      "Sending build context to Docker daemon  125.4kB\r",
      "\r\n",
      "Step 1/6 : FROM phenompeople/centos-python:3.6.3\n",
      " ---> e3d7d8ca4a30\n",
      "Step 2/6 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 1cbf2822780e\n",
      "Step 3/6 : ADD requirements.txt /\n",
      " ---> Using cache\n",
      " ---> d25d6314f799\n",
      "Step 4/6 : RUN pip3 install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 096da37bff72\n",
      "Step 5/6 : COPY src /opt/program\n",
      " ---> Using cache\n",
      " ---> 22d26313e1aa\n",
      "Step 6/6 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> 6ae53be99130\n",
      "Successfully built 6ae53be99130\n",
      "Successfully tagged keras-sagemaker-train:latest\n",
      "The push refers to repository [850021735523.dkr.ecr.us-east-1.amazonaws.com/keras-sagemaker-train]\n",
      "bb428fe28fc2: Preparing\n",
      "0dfc4f084219: Preparing\n",
      "6f5c0c46682a: Preparing\n",
      "952e0784686f: Preparing\n",
      "65c06ae44bbd: Preparing\n",
      "f194f1dd3e8f: Preparing\n",
      "ea264623c568: Preparing\n",
      "c4cd48200f79: Preparing\n",
      "bcc97fbfc9e1: Preparing\n",
      "ea264623c568: Waiting\n",
      "c4cd48200f79: Waiting\n",
      "bcc97fbfc9e1: Waiting\n",
      "f194f1dd3e8f: Waiting\n",
      "bb428fe28fc2: Pushed\n",
      "6f5c0c46682a: Pushed\n",
      "ea264623c568: Pushed\n",
      "952e0784686f: Pushed\n",
      "c4cd48200f79: Pushed\n",
      "65c06ae44bbd: Pushed\n",
      "bcc97fbfc9e1: Pushed\n",
      "f194f1dd3e8f: Pushed\n",
      "0dfc4f084219: Pushed\n",
      "latest: digest: sha256:79484d900a9ec7cba88bdf7c945faee63012a743967b0aeaa8cd2b802bd44e20 size: 2213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=keras-sagemaker-train\n",
    "\n",
    "chmod +x src/*\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "docker build  -t ${algorithm_name} -f Dockerfile.cpu .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data location - s3://sagemaker-keras-sagemaker-train/data\n",
      "output location - s3://sagemaker-keras-sagemaker-train/output\n"
     ]
    }
   ],
   "source": [
    "data_location = 's3://{}/data'.format(bucket)\n",
    "print(\"data location - \" + data_location)\n",
    "\n",
    "output_location = 's3://{}/output'.format(bucket)\n",
    "print(\"output location - \" + output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/keras-sagemaker-train'.format(account, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sage.estimator.Estimator(image_name=image, \n",
    "                                      role=role,\n",
    "                                      train_instance_count=1, \n",
    "                                      train_instance_type='ml.c5.2xlarge',\n",
    "                                      output_path=output_location,\n",
    "                                      sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-07 07:37:31 Starting - Starting the training job...\n",
      "2019-06-07 07:37:33 Starting - Launching requested ML instances......\n",
      "2019-06-07 07:38:42 Starting - Preparing the instances for training...\n",
      "2019-06-07 07:39:21 Downloading - Downloading input data...\n",
      "2019-06-07 07:39:33 Training - Downloading the training image...\n",
      "2019-06-07 07:40:31 Uploading - Uploading generated training model\n",
      "\u001b[31mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mColocations handled automatically by placer.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mUse tf.cast instead.\u001b[0m\n",
      "\u001b[31m2019-06-07 07:40:17.207731: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[31m2019-06-07 07:40:17.249781: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\u001b[0m\n",
      "\u001b[31m2019-06-07 07:40:17.251608: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x126118d0 executing computations on platform Host. Devices:\u001b[0m\n",
      "\u001b[31m2019-06-07 07:40:17.251626: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\u001b[0m\n",
      "\u001b[31mScript Status - Starting\u001b[0m\n",
      "\u001b[31mNumber of data samples:  10000\u001b[0m\n",
      "\u001b[31mNumber of data labels:  10000\u001b[0m\n",
      "\u001b[31mNumber of training samples:  ((8000, 784), (8000, 10))\u001b[0m\n",
      "\u001b[31mNumber of test samples:  ((2000, 784), (2000, 10))\u001b[0m\n",
      "\u001b[31mFinished reading the data.\u001b[0m\n",
      "\u001b[31mStarting the model training\u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[31m=================================================================\u001b[0m\n",
      "\u001b[31mdense_1 (Dense)              (None, 256)               200960    \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdropout_1 (Dropout)          (None, 256)               0         \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdense_2 (Dense)              (None, 64)                16448     \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdropout_2 (Dropout)          (None, 64)                0         \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdense_3 (Dense)              (None, 16)                1040      \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdense_4 (Dense)              (None, 10)                170       \u001b[0m\n",
      "\u001b[31m=================================================================\u001b[0m\n",
      "\u001b[31mTotal params: 218,618\u001b[0m\n",
      "\u001b[31mTrainable params: 218,618\u001b[0m\n",
      "\u001b[31mNon-trainable params: 0\u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mTrain on 8000 samples, validate on 2000 samples\u001b[0m\n",
      "\u001b[31mEpoch 1/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 21s - loss: 2.3023 - acc: 0.0781\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 1s - loss: 2.3026 - acc: 0.1115 \u001b[0m\n",
      "\u001b[31m3840/8000 [=============>................] - ETA: 0s - loss: 2.3003 - acc: 0.1164\u001b[0m\n",
      "\u001b[31m5760/8000 [====================>.........] - ETA: 0s - loss: 2.2982 - acc: 0.1198\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 2.2958 - acc: 0.1267\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 1s 76us/step - loss: 2.2952 - acc: 0.1278 - val_loss: 2.2831 - val_acc: 0.2815\u001b[0m\n",
      "\u001b[31mEpoch 2/12\n",
      "\u001b[0m\n",
      "\u001b[31m 128/8000 [..............................] - ETA: 0s - loss: 2.2824 - acc: 0.2109\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 2.2771 - acc: 0.1699\u001b[0m\n",
      "\u001b[31m3840/8000 [=============>................] - ETA: 0s - loss: 2.2695 - acc: 0.1714\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 2.2625 - acc: 0.1800\u001b[0m\n",
      "\u001b[31m7552/8000 [===========================>..] - ETA: 0s - loss: 2.2503 - acc: 0.1920\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 31us/step - loss: 2.2479 - acc: 0.1938 - val_loss: 2.1980 - val_acc: 0.2265\u001b[0m\n",
      "\u001b[31mEpoch 3/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 2.2093 - acc: 0.1875\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 2.1996 - acc: 0.1797\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 2.1736 - acc: 0.1968\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 2.1766 - acc: 0.1938\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 2.1552 - acc: 0.2112\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 29us/step - loss: 2.1520 - acc: 0.2144 - val_loss: 2.0484 - val_acc: 0.2305\u001b[0m\n",
      "\u001b[31mEpoch 4/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.9975 - acc: 0.2500\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 2.0514 - acc: 0.2222\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 2.0210 - acc: 0.2424\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 2.0062 - acc: 0.2559\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 1.9873 - acc: 0.2672\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 30us/step - loss: 1.9821 - acc: 0.2705 - val_loss: 1.8383 - val_acc: 0.3105\u001b[0m\n",
      "\u001b[31mEpoch 5/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.8571 - acc: 0.3281\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 1.9114 - acc: 0.2920\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 1.8556 - acc: 0.3213\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 1.8282 - acc: 0.3283\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 1.8113 - acc: 0.3361\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 29us/step - loss: 1.8072 - acc: 0.3376 - val_loss: 1.5419 - val_acc: 0.3930\u001b[0m\n",
      "\u001b[31mEpoch 6/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.5839 - acc: 0.3594\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 1.5889 - acc: 0.4341\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 1.5871 - acc: 0.4327\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 1.5994 - acc: 0.4258\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 1.5867 - acc: 0.4283\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 30us/step - loss: 1.5852 - acc: 0.4288 - val_loss: 1.8698 - val_acc: 0.2635\u001b[0m\n",
      "\u001b[31mEpoch 7/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.9565 - acc: 0.2188\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 1.3854 - acc: 0.5000\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 1.4092 - acc: 0.4854\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 1.3734 - acc: 0.5017\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 1.3708 - acc: 0.5063\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 29us/step - loss: 1.3634 - acc: 0.5109 - val_loss: 1.1750 - val_acc: 0.6110\u001b[0m\n",
      "\u001b[31mEpoch 8/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.2494 - acc: 0.6094\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 1.1647 - acc: 0.6060\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 1.1137 - acc: 0.6210\u001b[0m\n",
      "\u001b[31m5760/8000 [====================>.........] - ETA: 0s - loss: 1.1361 - acc: 0.6038\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 1.1080 - acc: 0.6105\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 29us/step - loss: 1.1133 - acc: 0.6069 - val_loss: 1.1272 - val_acc: 0.5605\u001b[0m\n",
      "\u001b[31mEpoch 9/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.2338 - acc: 0.5469\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 1.1315 - acc: 0.5840\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 1.0679 - acc: 0.6217\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 1.0250 - acc: 0.6359\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 1.0202 - acc: 0.6391\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 29us/step - loss: 1.0191 - acc: 0.6396 - val_loss: 1.0597 - val_acc: 0.6025\u001b[0m\n",
      "\u001b[31mEpoch 10/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.2537 - acc: 0.5547\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 1.0060 - acc: 0.6426\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.9551 - acc: 0.6656\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 0.9375 - acc: 0.6746\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 0.9299 - acc: 0.6715\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 29us/step - loss: 0.9308 - acc: 0.6710 - val_loss: 0.8684 - val_acc: 0.6815\u001b[0m\n",
      "\u001b[31mEpoch 11/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.0483 - acc: 0.6406\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.8703 - acc: 0.6979\u001b[0m\n",
      "\u001b[31m3712/8000 [============>.................] - ETA: 0s - loss: 0.8466 - acc: 0.7139\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 0.8580 - acc: 0.7031\u001b[0m\n",
      "\u001b[31m7552/8000 [===========================>..] - ETA: 0s - loss: 0.8371 - acc: 0.7152\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 30us/step - loss: 0.8316 - acc: 0.7170 - val_loss: 0.6726 - val_acc: 0.8000\u001b[0m\n",
      "\u001b[31mEpoch 12/12\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.9903 - acc: 0.6562\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.7930 - acc: 0.7354\u001b[0m\n",
      "\u001b[31m3712/8000 [============>.................] - ETA: 0s - loss: 0.7751 - acc: 0.7406\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 0.7756 - acc: 0.7429\u001b[0m\n",
      "\u001b[31m7552/8000 [===========================>..] - ETA: 0s - loss: 0.7834 - acc: 0.7354\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 30us/step - loss: 0.7782 - acc: 0.7370 - val_loss: 0.6147 - val_acc: 0.8215\u001b[0m\n",
      "\u001b[31mTest loss: 0.6147462196350097\u001b[0m\n",
      "\u001b[31mTest accuracy: 0.8215\u001b[0m\n",
      "\u001b[31mFinished training the model.\u001b[0m\n",
      "\u001b[31mScript Status - Finished\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-06-07 07:40:37 Completed - Training job completed\n",
      "Billable seconds: 76\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
