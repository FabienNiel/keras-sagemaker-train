{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training Job \n",
    "\n",
    "### Please go through this notebook only if you have finished Part 1 to Part 4 of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 1: Import packages, get IAM role, get the region and set the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket ='keras-sagemaker-train' # Put your s3 bucket name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 2: Create the algorithm image and push to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Stopping docker: [  OK  ]\r\n",
      "Starting docker:\t.[  OK  ]\r\n",
      "Sending build context to Docker daemon  211.1MB\r",
      "\r\n",
      "Step 1/6 : FROM phenompeople/centos-python:3.6.3\n",
      " ---> e3d7d8ca4a30\n",
      "Step 2/6 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> b365aa41218a\n",
      "Step 3/6 : ADD requirements-cpu.txt /\n",
      " ---> Using cache\n",
      " ---> 3c3af67525fe\n",
      "Step 4/6 : RUN pip3 install -r requirements-cpu.txt\n",
      " ---> Using cache\n",
      " ---> c6adb5f69019\n",
      "Step 5/6 : COPY src /opt/program\n",
      " ---> Using cache\n",
      " ---> 902eafc56ddd\n",
      "Step 6/6 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> 633aa8bfbd9d\n",
      "Successfully built 633aa8bfbd9d\n",
      "Successfully tagged keras-sagemaker-train:latest\n",
      "The push refers to repository [850021735523.dkr.ecr.us-east-1.amazonaws.com/keras-sagemaker-train]\n",
      "bfc4f2733525: Preparing\n",
      "72ff4d93b480: Preparing\n",
      "50b4f5bbfa32: Preparing\n",
      "952e0784686f: Preparing\n",
      "65c06ae44bbd: Preparing\n",
      "f194f1dd3e8f: Preparing\n",
      "ea264623c568: Preparing\n",
      "c4cd48200f79: Preparing\n",
      "bcc97fbfc9e1: Preparing\n",
      "f194f1dd3e8f: Waiting\n",
      "ea264623c568: Waiting\n",
      "bcc97fbfc9e1: Waiting\n",
      "c4cd48200f79: Waiting\n",
      "65c06ae44bbd: Layer already exists\n",
      "952e0784686f: Layer already exists\n",
      "ea264623c568: Layer already exists\n",
      "f194f1dd3e8f: Layer already exists\n",
      "c4cd48200f79: Layer already exists\n",
      "bcc97fbfc9e1: Layer already exists\n",
      "bfc4f2733525: Pushed\n",
      "50b4f5bbfa32: Pushed\n",
      "72ff4d93b480: Pushed\n",
      "latest: digest: sha256:3497c8a95ba122d054f3e84734c55957349a2928c0296b741d5e9c6ade8a651e size: 2213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=keras-sagemaker-train\n",
    "\n",
    "chmod +x src/*\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "# Comment the line below to use a GPU\n",
    "docker build  -t ${algorithm_name} -f Dockerfile.cpu .\n",
    "\n",
    "# Uncomment the below line if you wish to run on a GPU\n",
    "#docker build  -t ${algorithm_name} -f Dockerfile.gpu . \n",
    "\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 3: Define variables with data location and output location in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data location - s3://keras-sagemaker-train/data\n",
      "output location - s3://keras-sagemaker-train/output\n"
     ]
    }
   ],
   "source": [
    "data_location = 's3://{}/data'.format(bucket)\n",
    "print(\"data location - \" + data_location)\n",
    "\n",
    "output_location = 's3://{}/output'.format(bucket)\n",
    "print(\"output location - \" + output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 4: Create a SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 5: Define variables for account, region and algorithm image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account'] # aws account \n",
    "region = sess.boto_session.region_name # aws server region\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/keras-sagemaker-train'.format(account, region) # algorithm image path in ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 6: Define hyperparameters to be passed to your algorithm. \n",
    "In this project we are reading two hyperparameters for training. Use of hyperparameters in optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"batch_size\":128, \"epochs\":30}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 7: Create the training job using SageMaker Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sage.estimator.Estimator(image_name=image, \n",
    "                                      role=role,\n",
    "                                      train_instance_count=1, \n",
    "                                      train_instance_type='ml.c5.2xlarge',\n",
    "                                      hyperparameters=hyperparameters,\n",
    "                                      output_path=output_location,\n",
    "                                      sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 8: Run the training job by passing the data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-14 05:11:15 Starting - Starting the training job...\n",
      "2019-06-14 05:11:17 Starting - Launching requested ML instances......\n",
      "2019-06-14 05:12:26 Starting - Preparing the instances for training......\n",
      "2019-06-14 05:13:36 Downloading - Downloading input data\n",
      "2019-06-14 05:13:36 Training - Downloading the training image...\n",
      "2019-06-14 05:14:16 Training - Training image download completed. Training in progress..\n",
      "\u001b[31m2019-06-14 05:14:18.178636: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[31m2019-06-14 05:14:18.210980: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\u001b[0m\n",
      "\u001b[31m2019-06-14 05:14:18.212464: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x42fa440 executing computations on platform Host. Devices:\u001b[0m\n",
      "\u001b[31m2019-06-14 05:14:18.212487: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\u001b[0m\n",
      "\u001b[31mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mColocations handled automatically by placer.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mUse tf.cast instead.\u001b[0m\n",
      "\u001b[31m[name: \"/device:CPU:0\"\u001b[0m\n",
      "\u001b[31mdevice_type: \"CPU\"\u001b[0m\n",
      "\u001b[31mmemory_limit: 268435456\u001b[0m\n",
      "\u001b[31mlocality {\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31mincarnation: 1403856113116283786\u001b[0m\n",
      "\u001b[31m, name: \"/device:XLA_CPU:0\"\u001b[0m\n",
      "\u001b[31mdevice_type: \"XLA_CPU\"\u001b[0m\n",
      "\u001b[31mmemory_limit: 17179869184\u001b[0m\n",
      "\u001b[31mlocality {\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31mincarnation: 5958035354186208720\u001b[0m\n",
      "\u001b[31mphysical_device_desc: \"device: XLA_CPU device\"\u001b[0m\n",
      "\u001b[31m]\u001b[0m\n",
      "\u001b[31mScript Status - Starting\u001b[0m\n",
      "\u001b[31mReading hyperparameters\u001b[0m\n",
      "\u001b[31mNumber of data samples:  10000\u001b[0m\n",
      "\u001b[31mNumber of data labels:  10000\u001b[0m\n",
      "\u001b[31mNumber of training samples:  ((8000, 784), (8000, 10))\u001b[0m\n",
      "\u001b[31mNumber of test samples:  ((2000, 784), (2000, 10))\u001b[0m\n",
      "\u001b[31mFinished reading the data.\u001b[0m\n",
      "\u001b[31mStarting the model training\u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[31m=================================================================\u001b[0m\n",
      "\u001b[31mdense_1 (Dense)              (None, 512)               401920    \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdropout_1 (Dropout)          (None, 512)               0         \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdense_2 (Dense)              (None, 512)               262656    \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdropout_2 (Dropout)          (None, 512)               0         \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdense_3 (Dense)              (None, 10)                5130      \u001b[0m\n",
      "\u001b[31m=================================================================\u001b[0m\n",
      "\u001b[31mTotal params: 669,706\u001b[0m\n",
      "\u001b[31mTrainable params: 669,706\u001b[0m\n",
      "\u001b[31mNon-trainable params: 0\u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mTrain on 8000 samples, validate on 2000 samples\u001b[0m\n",
      "\u001b[31mEpoch 1/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 13s - loss: 2.3025 - acc: 0.0938\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 1s - loss: 2.2931 - acc: 0.1279 \u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 2.2740 - acc: 0.1792\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 2.2368 - acc: 0.2715\u001b[0m\n",
      "\u001b[31m4096/8000 [==============>...............] - ETA: 0s - loss: 2.1885 - acc: 0.3049\u001b[0m\n",
      "\u001b[31m5120/8000 [==================>...........] - ETA: 0s - loss: 2.1290 - acc: 0.3412\u001b[0m\n",
      "\u001b[31m6144/8000 [======================>.......] - ETA: 0s - loss: 2.0645 - acc: 0.3690\u001b[0m\n",
      "\u001b[31m7168/8000 [=========================>....] - ETA: 0s - loss: 1.9979 - acc: 0.3910\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 1s 86us/step - loss: 1.9429 - acc: 0.4099 - val_loss: 1.3639 - val_acc: 0.6520\u001b[0m\n",
      "\u001b[31mEpoch 2/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.4856 - acc: 0.5703\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 1.3734 - acc: 0.5825\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 1.3117 - acc: 0.6103\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 1.2712 - acc: 0.6194\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 1.2258 - acc: 0.6380\u001b[0m\n",
      "\u001b[31m5120/8000 [==================>...........] - ETA: 0s - loss: 1.1930 - acc: 0.6430\u001b[0m\n",
      "\u001b[31m6016/8000 [=====================>........] - ETA: 0s - loss: 1.1621 - acc: 0.6491\u001b[0m\n",
      "\u001b[31m7040/8000 [=========================>....] - ETA: 0s - loss: 1.1315 - acc: 0.6580\u001b[0m\n",
      "\u001b[31m7936/8000 [============================>.] - ETA: 0s - loss: 1.1125 - acc: 0.6609\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 59us/step - loss: 1.1101 - acc: 0.6617 - val_loss: 0.8166 - val_acc: 0.7605\u001b[0m\n",
      "\u001b[31mEpoch 3/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.7739 - acc: 0.7031\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.8701 - acc: 0.7135\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.8710 - acc: 0.7134\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.8628 - acc: 0.7174\u001b[0m\n",
      "\u001b[31m4096/8000 [==============>...............] - ETA: 0s - loss: 0.8470 - acc: 0.7271\u001b[0m\n",
      "\u001b[31m5120/8000 [==================>...........] - ETA: 0s - loss: 0.8301 - acc: 0.7344\u001b[0m\n",
      "\u001b[31m6144/8000 [======================>.......] - ETA: 0s - loss: 0.8218 - acc: 0.7350\u001b[0m\n",
      "\u001b[31m7168/8000 [=========================>....] - ETA: 0s - loss: 0.8101 - acc: 0.7409\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 59us/step - loss: 0.8086 - acc: 0.7414 - val_loss: 0.6037 - val_acc: 0.8405\u001b[0m\n",
      "\u001b[31mEpoch 4/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.7493 - acc: 0.7344\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.7163 - acc: 0.7674\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.7261 - acc: 0.7748\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.7001 - acc: 0.7829\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.6883 - acc: 0.7838\u001b[0m\n",
      "\u001b[31m4992/8000 [=================>............] - ETA: 0s - loss: 0.6730 - acc: 0.7873\u001b[0m\n",
      "\u001b[31m6016/8000 [=====================>........] - ETA: 0s - loss: 0.6734 - acc: 0.7857\u001b[0m\n",
      "\u001b[31m7040/8000 [=========================>....] - ETA: 0s - loss: 0.6665 - acc: 0.7898\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 59us/step - loss: 0.6629 - acc: 0.7914 - val_loss: 0.4919 - val_acc: 0.8705\u001b[0m\n",
      "\u001b[31mEpoch 5/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.6484 - acc: 0.7734\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.5919 - acc: 0.8252\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.6070 - acc: 0.8198\u001b[0m\n",
      "\u001b[31m2816/8000 [=========>....................] - ETA: 0s - loss: 0.6044 - acc: 0.8171\u001b[0m\n",
      "\u001b[31m3712/8000 [============>.................] - ETA: 0s - loss: 0.5975 - acc: 0.8203\u001b[0m\n",
      "\u001b[31m4608/8000 [================>.............] - ETA: 0s - loss: 0.5866 - acc: 0.8214\u001b[0m\n",
      "\u001b[31m5504/8000 [===================>..........] - ETA: 0s - loss: 0.5841 - acc: 0.8194\u001b[0m\n",
      "\u001b[31m6528/8000 [=======================>......] - ETA: 0s - loss: 0.5794 - acc: 0.8235\u001b[0m\n",
      "\u001b[31m7552/8000 [===========================>..] - ETA: 0s - loss: 0.5697 - acc: 0.8263\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 61us/step - loss: 0.5665 - acc: 0.8275 - val_loss: 0.4021 - val_acc: 0.8945\u001b[0m\n",
      "\u001b[31mEpoch 6/30\n",
      "\u001b[0m\n",
      "\u001b[31m 128/8000 [..............................] - ETA: 0s - loss: 0.6161 - acc: 0.8281\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.5020 - acc: 0.8359\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.5229 - acc: 0.8311\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.5207 - acc: 0.8392\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.5199 - acc: 0.8377\u001b[0m\n",
      "\u001b[31m4864/8000 [=================>............] - ETA: 0s - loss: 0.5095 - acc: 0.8421\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 0.5122 - acc: 0.8415\u001b[0m\n",
      "\u001b[31m6912/8000 [========================>.....] - ETA: 0s - loss: 0.5005 - acc: 0.8459\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 0.4964 - acc: 0.8478\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.4947 - acc: 0.8488 - val_loss: 0.3755 - val_acc: 0.8885\u001b[0m\n",
      "\u001b[31mEpoch 7/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4013 - acc: 0.8750\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.5176 - acc: 0.8271\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.4928 - acc: 0.8422\u001b[0m\n",
      "\u001b[31m2816/8000 [=========>....................] - ETA: 0s - loss: 0.4715 - acc: 0.8505\u001b[0m\n",
      "\u001b[31m3712/8000 [============>.................] - ETA: 0s - loss: 0.4690 - acc: 0.8529\u001b[0m\n",
      "\u001b[31m4608/8000 [================>.............] - ETA: 0s - loss: 0.4605 - acc: 0.8579\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 0.4556 - acc: 0.8617\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 0.4504 - acc: 0.8618\u001b[0m\n",
      "\u001b[31m7552/8000 [===========================>..] - ETA: 0s - loss: 0.4486 - acc: 0.8618\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 61us/step - loss: 0.4479 - acc: 0.8628 - val_loss: 0.3135 - val_acc: 0.9120\u001b[0m\n",
      "\u001b[31mEpoch 8/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3527 - acc: 0.8828\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.3836 - acc: 0.8818\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.4160 - acc: 0.8703\u001b[0m\n",
      "\u001b[31m2944/8000 [==========>...................] - ETA: 0s - loss: 0.4142 - acc: 0.8730\u001b[0m\n",
      "\u001b[31m3840/8000 [=============>................] - ETA: 0s - loss: 0.4065 - acc: 0.8747\u001b[0m\n",
      "\u001b[31m4736/8000 [================>.............] - ETA: 0s - loss: 0.4038 - acc: 0.8746\u001b[0m\n",
      "\u001b[31m5760/8000 [====================>.........] - ETA: 0s - loss: 0.4043 - acc: 0.8736\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 0.4094 - acc: 0.8715\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.4116 - acc: 0.8715\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 59us/step - loss: 0.4107 - acc: 0.8719 - val_loss: 0.2991 - val_acc: 0.9215\u001b[0m\n",
      "\u001b[31mEpoch 9/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3146 - acc: 0.8906\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3632 - acc: 0.8941\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.3846 - acc: 0.8860\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.3882 - acc: 0.8828\u001b[0m\n",
      "\u001b[31m4096/8000 [==============>...............] - ETA: 0s - loss: 0.3760 - acc: 0.8889\u001b[0m\n",
      "\u001b[31m5120/8000 [==================>...........] - ETA: 0s - loss: 0.3853 - acc: 0.8859\u001b[0m\n",
      "\u001b[31m6144/8000 [======================>.......] - ETA: 0s - loss: 0.3817 - acc: 0.8840\u001b[0m\n",
      "\u001b[31m7040/8000 [=========================>....] - ETA: 0s - loss: 0.3783 - acc: 0.8847\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 59us/step - loss: 0.3781 - acc: 0.8840 - val_loss: 0.2955 - val_acc: 0.9220\u001b[0m\n",
      "\u001b[31mEpoch 10/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4007 - acc: 0.8672\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3506 - acc: 0.8967\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.3466 - acc: 0.8916\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.3325 - acc: 0.8988\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.3425 - acc: 0.8949\u001b[0m\n",
      "\u001b[31m4864/8000 [=================>............] - ETA: 0s - loss: 0.3447 - acc: 0.8951\u001b[0m\n",
      "\u001b[31m5760/8000 [====================>.........] - ETA: 0s - loss: 0.3459 - acc: 0.8936\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 0.3491 - acc: 0.8924\u001b[0m\n",
      "\u001b[31m7552/8000 [===========================>..] - ETA: 0s - loss: 0.3499 - acc: 0.8933\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.3512 - acc: 0.8937 - val_loss: 0.2602 - val_acc: 0.9290\u001b[0m\n",
      "\u001b[31mEpoch 11/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3963 - acc: 0.8906\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3548 - acc: 0.8915\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.3285 - acc: 0.9021\u001b[0m\n",
      "\u001b[31m2944/8000 [==========>...................] - ETA: 0s - loss: 0.3266 - acc: 0.9008\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.3386 - acc: 0.8954\u001b[0m\n",
      "\u001b[31m4864/8000 [=================>............] - ETA: 0s - loss: 0.3393 - acc: 0.8943\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 0.3412 - acc: 0.8945\u001b[0m\n",
      "\u001b[31m6784/8000 [========================>.....] - ETA: 0s - loss: 0.3388 - acc: 0.8974\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8969\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 61us/step - loss: 0.3364 - acc: 0.8971 - val_loss: 0.2689 - val_acc: 0.9295\u001b[0m\n",
      "\u001b[31mEpoch 12/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3131 - acc: 0.9141\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.3211 - acc: 0.9111\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.3267 - acc: 0.9072\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.3307 - acc: 0.9046\u001b[0m\n",
      "\u001b[31m4096/8000 [==============>...............] - ETA: 0s - loss: 0.3231 - acc: 0.9062\u001b[0m\n",
      "\u001b[31m5120/8000 [==================>...........] - ETA: 0s - loss: 0.3295 - acc: 0.9047\u001b[0m\n",
      "\u001b[31m6144/8000 [======================>.......] - ETA: 0s - loss: 0.3253 - acc: 0.9045\u001b[0m\n",
      "\u001b[31m7040/8000 [=========================>....] - ETA: 0s - loss: 0.3208 - acc: 0.9050\u001b[0m\n",
      "\u001b[31m7936/8000 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.9037\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.3197 - acc: 0.9038 - val_loss: 0.2467 - val_acc: 0.9310\u001b[0m\n",
      "\u001b[31mEpoch 13/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2482 - acc: 0.9297\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.3065 - acc: 0.8965\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.2917 - acc: 0.9057\u001b[0m\n",
      "\u001b[31m2944/8000 [==========>...................] - ETA: 0s - loss: 0.2936 - acc: 0.9090\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.3057 - acc: 0.9078\u001b[0m\n",
      "\u001b[31m4864/8000 [=================>............] - ETA: 0s - loss: 0.3069 - acc: 0.9069\u001b[0m\n",
      "\u001b[31m5760/8000 [====================>.........] - ETA: 0s - loss: 0.3105 - acc: 0.9064\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 0.3050 - acc: 0.9081\u001b[0m\n",
      "\u001b[31m7552/8000 [===========================>..] - ETA: 0s - loss: 0.3029 - acc: 0.9078\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.3011 - acc: 0.9085 - val_loss: 0.2183 - val_acc: 0.9380\u001b[0m\n",
      "\u001b[31mEpoch 14/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3481 - acc: 0.8828\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2605 - acc: 0.9209\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.2742 - acc: 0.9136\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.2853 - acc: 0.9102\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.2853 - acc: 0.9095\u001b[0m\n",
      "\u001b[31m4992/8000 [=================>............] - ETA: 0s - loss: 0.2915 - acc: 0.9099\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 0.2922 - acc: 0.9095\u001b[0m\n",
      "\u001b[31m6784/8000 [========================>.....] - ETA: 0s - loss: 0.2909 - acc: 0.9095\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9105\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 59us/step - loss: 0.2884 - acc: 0.9103 - val_loss: 0.2333 - val_acc: 0.9310\u001b[0m\n",
      "\u001b[31mEpoch 15/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3501 - acc: 0.9062\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2336 - acc: 0.9277\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.2391 - acc: 0.9250\u001b[0m\n",
      "\u001b[31m2944/8000 [==========>...................] - ETA: 0s - loss: 0.2557 - acc: 0.9212\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.2567 - acc: 0.9201\u001b[0m\n",
      "\u001b[31m4864/8000 [=================>............] - ETA: 0s - loss: 0.2713 - acc: 0.9171\u001b[0m\n",
      "\u001b[31m5760/8000 [====================>.........] - ETA: 0s - loss: 0.2774 - acc: 0.9148\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 0.2751 - acc: 0.9154\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2729 - acc: 0.9163\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 61us/step - loss: 0.2748 - acc: 0.9160 - val_loss: 0.2125 - val_acc: 0.9330\u001b[0m\n",
      "\u001b[31mEpoch 16/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3444 - acc: 0.8984\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2455 - acc: 0.9277\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.2534 - acc: 0.9204\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.2455 - acc: 0.9238\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.2513 - acc: 0.9231\u001b[0m\n",
      "\u001b[31m4864/8000 [=================>............] - ETA: 0s - loss: 0.2573 - acc: 0.9213\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 0.2571 - acc: 0.9231\u001b[0m\n",
      "\u001b[31m6784/8000 [========================>.....] - ETA: 0s - loss: 0.2570 - acc: 0.9213\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 0.2584 - acc: 0.9210\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.2609 - acc: 0.9200 - val_loss: 0.1954 - val_acc: 0.9435\u001b[0m\n",
      "\u001b[31mEpoch 17/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1950 - acc: 0.9219\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2369 - acc: 0.9245\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2384 - acc: 0.9251\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.2520 - acc: 0.9235\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.2464 - acc: 0.9262\u001b[0m\n",
      "\u001b[31m4992/8000 [=================>............] - ETA: 0s - loss: 0.2436 - acc: 0.9267\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 0.2444 - acc: 0.9270\u001b[0m\n",
      "\u001b[31m6784/8000 [========================>.....] - ETA: 0s - loss: 0.2491 - acc: 0.9245\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9243\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.2470 - acc: 0.9248 - val_loss: 0.2068 - val_acc: 0.9425\u001b[0m\n",
      "\u001b[31mEpoch 18/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3296 - acc: 0.8828\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2072 - acc: 0.9340\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2312 - acc: 0.9320\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.2442 - acc: 0.9281\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.2391 - acc: 0.9285\u001b[0m\n",
      "\u001b[31m5120/8000 [==================>...........] - ETA: 0s - loss: 0.2433 - acc: 0.9281\u001b[0m\n",
      "\u001b[31m6016/8000 [=====================>........] - ETA: 0s - loss: 0.2426 - acc: 0.9285\u001b[0m\n",
      "\u001b[31m7040/8000 [=========================>....] - ETA: 0s - loss: 0.2380 - acc: 0.9294\u001b[0m\n",
      "\u001b[31m7936/8000 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9278\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.2388 - acc: 0.9279 - val_loss: 0.1860 - val_acc: 0.9500\u001b[0m\n",
      "\u001b[31mEpoch 19/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3497 - acc: 0.8984\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2216 - acc: 0.9424\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.2189 - acc: 0.9354\u001b[0m\n",
      "\u001b[31m2816/8000 [=========>....................] - ETA: 0s - loss: 0.2258 - acc: 0.9325\u001b[0m\n",
      "\u001b[31m3712/8000 [============>.................] - ETA: 0s - loss: 0.2252 - acc: 0.9305\u001b[0m\n",
      "\u001b[31m4736/8000 [================>.............] - ETA: 0s - loss: 0.2283 - acc: 0.9291\u001b[0m\n",
      "\u001b[31m5760/8000 [====================>.........] - ETA: 0s - loss: 0.2266 - acc: 0.9285\u001b[0m\n",
      "\u001b[31m6784/8000 [========================>.....] - ETA: 0s - loss: 0.2273 - acc: 0.9295\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2289 - acc: 0.9303\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 61us/step - loss: 0.2279 - acc: 0.9310 - val_loss: 0.1869 - val_acc: 0.9490\u001b[0m\n",
      "\u001b[31mEpoch 20/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1228 - acc: 0.9766\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2085 - acc: 0.9323\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.2064 - acc: 0.9316\u001b[0m\n",
      "\u001b[31m2944/8000 [==========>...................] - ETA: 0s - loss: 0.2167 - acc: 0.9304\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.2096 - acc: 0.9337\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-06-14 05:14:47 Uploading - Uploading generated training model\n",
      "2019-06-14 05:14:47 Completed - Training job completed\n",
      "\u001b[31m4864/8000 [=================>............] - ETA: 0s - loss: 0.2098 - acc: 0.9359\u001b[0m\n",
      "\u001b[31m5760/8000 [====================>.........] - ETA: 0s - loss: 0.2153 - acc: 0.9342\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 0.2200 - acc: 0.9333\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2209 - acc: 0.9322\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.2215 - acc: 0.9323 - val_loss: 0.1843 - val_acc: 0.9485\u001b[0m\n",
      "\u001b[31mEpoch 21/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1346 - acc: 0.9766\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2094 - acc: 0.9384\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.1902 - acc: 0.9458\u001b[0m\n",
      "\u001b[31m2944/8000 [==========>...................] - ETA: 0s - loss: 0.1922 - acc: 0.9412\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.1945 - acc: 0.9413\u001b[0m\n",
      "\u001b[31m4992/8000 [=================>............] - ETA: 0s - loss: 0.2085 - acc: 0.9361\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 0.2072 - acc: 0.9373\u001b[0m\n",
      "\u001b[31m6784/8000 [========================>.....] - ETA: 0s - loss: 0.2092 - acc: 0.9363\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2105 - acc: 0.9357\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.2108 - acc: 0.9359 - val_loss: 0.1855 - val_acc: 0.9475\u001b[0m\n",
      "\u001b[31mEpoch 22/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2559 - acc: 0.9297\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2043 - acc: 0.9346\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.1947 - acc: 0.9409\u001b[0m\n",
      "\u001b[31m2944/8000 [==========>...................] - ETA: 0s - loss: 0.1958 - acc: 0.9351\u001b[0m\n",
      "\u001b[31m3840/8000 [=============>................] - ETA: 0s - loss: 0.1897 - acc: 0.9388\u001b[0m\n",
      "\u001b[31m4736/8000 [================>.............] - ETA: 0s - loss: 0.1999 - acc: 0.9362\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 0.2004 - acc: 0.9366\u001b[0m\n",
      "\u001b[31m6528/8000 [=======================>......] - ETA: 0s - loss: 0.1983 - acc: 0.9380\u001b[0m\n",
      "\u001b[31m7552/8000 [===========================>..] - ETA: 0s - loss: 0.1978 - acc: 0.9383\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 61us/step - loss: 0.1990 - acc: 0.9380 - val_loss: 0.1891 - val_acc: 0.9445\u001b[0m\n",
      "\u001b[31mEpoch 23/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2356 - acc: 0.9062\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1846 - acc: 0.9424\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.1914 - acc: 0.9422\u001b[0m\n",
      "\u001b[31m2816/8000 [=========>....................] - ETA: 0s - loss: 0.1786 - acc: 0.9464\u001b[0m\n",
      "\u001b[31m3712/8000 [============>.................] - ETA: 0s - loss: 0.1798 - acc: 0.9459\u001b[0m\n",
      "\u001b[31m4608/8000 [================>.............] - ETA: 0s - loss: 0.1877 - acc: 0.9436\u001b[0m\n",
      "\u001b[31m5504/8000 [===================>..........] - ETA: 0s - loss: 0.1877 - acc: 0.9440\u001b[0m\n",
      "\u001b[31m6400/8000 [=======================>......] - ETA: 0s - loss: 0.1885 - acc: 0.9430\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.1910 - acc: 0.9426\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 61us/step - loss: 0.1909 - acc: 0.9417 - val_loss: 0.1879 - val_acc: 0.9480\u001b[0m\n",
      "\u001b[31mEpoch 24/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1556 - acc: 0.9609\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1455 - acc: 0.9521\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.1549 - acc: 0.9469\u001b[0m\n",
      "\u001b[31m2944/8000 [==========>...................] - ETA: 0s - loss: 0.1600 - acc: 0.9460\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.1596 - acc: 0.9486\u001b[0m\n",
      "\u001b[31m4992/8000 [=================>............] - ETA: 0s - loss: 0.1746 - acc: 0.9459\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 0.1720 - acc: 0.9470\u001b[0m\n",
      "\u001b[31m6784/8000 [========================>.....] - ETA: 0s - loss: 0.1747 - acc: 0.9459\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.1775 - acc: 0.9452\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.1792 - acc: 0.9449 - val_loss: 0.1656 - val_acc: 0.9550\u001b[0m\n",
      "\u001b[31mEpoch 25/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1835 - acc: 0.9688\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1762 - acc: 0.9482\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.1708 - acc: 0.9510\u001b[0m\n",
      "\u001b[31m2816/8000 [=========>....................] - ETA: 0s - loss: 0.1756 - acc: 0.9471\u001b[0m\n",
      "\u001b[31m3712/8000 [============>.................] - ETA: 0s - loss: 0.1746 - acc: 0.9464\u001b[0m\n",
      "\u001b[31m4736/8000 [================>.............] - ETA: 0s - loss: 0.1797 - acc: 0.9447\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 0.1758 - acc: 0.9458\u001b[0m\n",
      "\u001b[31m6528/8000 [=======================>......] - ETA: 0s - loss: 0.1778 - acc: 0.9447\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.1770 - acc: 0.9460\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 61us/step - loss: 0.1770 - acc: 0.9457 - val_loss: 0.1597 - val_acc: 0.9560\u001b[0m\n",
      "\u001b[31mEpoch 26/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1327 - acc: 0.9688\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1641 - acc: 0.9541\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.1718 - acc: 0.9502\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.1669 - acc: 0.9495\u001b[0m\n",
      "\u001b[31m3840/8000 [=============>................] - ETA: 0s - loss: 0.1625 - acc: 0.9508\u001b[0m\n",
      "\u001b[31m4736/8000 [================>.............] - ETA: 0s - loss: 0.1657 - acc: 0.9502\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 0.1660 - acc: 0.9503\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 0.1689 - acc: 0.9495\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.1687 - acc: 0.9490\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.1702 - acc: 0.9487 - val_loss: 0.1695 - val_acc: 0.9520\u001b[0m\n",
      "\u001b[31mEpoch 27/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1147 - acc: 0.9609\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1854 - acc: 0.9502\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.1571 - acc: 0.9556\u001b[0m\n",
      "\u001b[31m2944/8000 [==========>...................] - ETA: 0s - loss: 0.1686 - acc: 0.9511\u001b[0m\n",
      "\u001b[31m3840/8000 [=============>................] - ETA: 0s - loss: 0.1689 - acc: 0.9495\u001b[0m\n",
      "\u001b[31m4736/8000 [================>.............] - ETA: 0s - loss: 0.1668 - acc: 0.9487\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 0.1657 - acc: 0.9499\u001b[0m\n",
      "\u001b[31m6528/8000 [=======================>......] - ETA: 0s - loss: 0.1622 - acc: 0.9510\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.1611 - acc: 0.9508\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 61us/step - loss: 0.1607 - acc: 0.9511 - val_loss: 0.1657 - val_acc: 0.9540\u001b[0m\n",
      "\u001b[31mEpoch 28/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2380 - acc: 0.9219\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1576 - acc: 0.9549\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.1488 - acc: 0.9580\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.1545 - acc: 0.9551\u001b[0m\n",
      "\u001b[31m3968/8000 [=============>................] - ETA: 0s - loss: 0.1559 - acc: 0.9544\u001b[0m\n",
      "\u001b[31m4992/8000 [=================>............] - ETA: 0s - loss: 0.1571 - acc: 0.9527\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 0.1543 - acc: 0.9540\u001b[0m\n",
      "\u001b[31m6784/8000 [========================>.....] - ETA: 0s - loss: 0.1523 - acc: 0.9543\u001b[0m\n",
      "\u001b[31m7808/8000 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9539\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.1547 - acc: 0.9531 - val_loss: 0.1699 - val_acc: 0.9525\u001b[0m\n",
      "\u001b[31mEpoch 29/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1679 - acc: 0.9609\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1304 - acc: 0.9580\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.1334 - acc: 0.9547\u001b[0m\n",
      "\u001b[31m2816/8000 [=========>....................] - ETA: 0s - loss: 0.1421 - acc: 0.9535\u001b[0m\n",
      "\u001b[31m3840/8000 [=============>................] - ETA: 0s - loss: 0.1473 - acc: 0.9521\u001b[0m\n",
      "\u001b[31m4736/8000 [================>.............] - ETA: 0s - loss: 0.1493 - acc: 0.9531\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 0.1537 - acc: 0.9519\u001b[0m\n",
      "\u001b[31m6528/8000 [=======================>......] - ETA: 0s - loss: 0.1494 - acc: 0.9534\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.1483 - acc: 0.9537\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 61us/step - loss: 0.1472 - acc: 0.9541 - val_loss: 0.1538 - val_acc: 0.9575\u001b[0m\n",
      "\u001b[31mEpoch 30/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1012 - acc: 0.9609\u001b[0m\n",
      "\u001b[31m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1316 - acc: 0.9639\u001b[0m\n",
      "\u001b[31m1920/8000 [======>.......................] - ETA: 0s - loss: 0.1331 - acc: 0.9615\u001b[0m\n",
      "\u001b[31m2816/8000 [=========>....................] - ETA: 0s - loss: 0.1297 - acc: 0.9606\u001b[0m\n",
      "\u001b[31m3840/8000 [=============>................] - ETA: 0s - loss: 0.1357 - acc: 0.9591\u001b[0m\n",
      "\u001b[31m4864/8000 [=================>............] - ETA: 0s - loss: 0.1333 - acc: 0.9595\u001b[0m\n",
      "\u001b[31m5888/8000 [=====================>........] - ETA: 0s - loss: 0.1328 - acc: 0.9599\u001b[0m\n",
      "\u001b[31m6784/8000 [========================>.....] - ETA: 0s - loss: 0.1373 - acc: 0.9581\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.1383 - acc: 0.9579\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 60us/step - loss: 0.1402 - acc: 0.9569 - val_loss: 0.1488 - val_acc: 0.9590\u001b[0m\n",
      "\u001b[31mTest loss: 0.14877056488394738\u001b[0m\n",
      "\u001b[31mTest accuracy: 0.959\u001b[0m\n",
      "\u001b[31mFinished training the model.\u001b[0m\n",
      "\u001b[31mFinished training the model.\u001b[0m\n",
      "\u001b[31mScript Status - Finished\u001b[0m\n",
      "\u001b[31mTotal time taken to train the model:  20.779784202575684\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billable seconds: 89\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! We had a successful training job run in Amazon SageMaker.\n",
    "#### Please return to the tutorial for Part 6 where we will be running a training job in a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
